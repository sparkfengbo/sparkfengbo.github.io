<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://sparkfengbo.github.io</id>
    <title>FengBo`s Blog</title>
    <updated>2020-10-12T10:34:59.917Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://sparkfengbo.github.io"/>
    <link rel="self" href="https://sparkfengbo.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://sparkfengbo.github.io/images/avatar.png</logo>
    <icon>https://sparkfengbo.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, FengBo`s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Linux IO模型简介和IO多路复用]]></title>
        <id>https://sparkfengbo.github.io/post/czxt-linux-io-mo-xing-jian-jie-he-io-duo-lu-fu-yong/</id>
        <link href="https://sparkfengbo.github.io/post/czxt-linux-io-mo-xing-jian-jie-he-io-duo-lu-fu-yong/">
        </link>
        <updated>2020-10-12T10:33:34.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li>
<p><a href="https://www.jianshu.com/p/f8bc7199bb8e">Linux IO模型：阻塞/非阻塞/IO复用 同步/异步 Select/Epoll/AIO</a></p>
</li>
<li>
<p><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjA4MTExMw==&amp;mid=2247484746&amp;idx=1&amp;sn=c0a7f9129d780786cabfcac0a8aa6bb7&amp;source=41#wechat_redirect">漫话：如何给女朋友解释什么是Linux的五种IO模型？</a></p>
</li>
<li>
<p><a href="https://www.jianshu.com/p/dfd940e7fca2">聊聊IO多路复用之select、poll、epoll详解</a></p>
</li>
<li>
<p><a href="https://ring0.me/2014/11/sync-async-blocked/">大话同步/异步、阻塞/非阻塞</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/historyasamirror/article/details/5778378">IO - 同步，异步，阻塞，非阻塞 （亡羊补牢篇）</a></p>
</li>
<li>
<p>这篇文章图是中文的  <a href="https://www.jianshu.com/p/02f76566fd90">怎样理解阻塞非阻塞与同步异步的区别？</a></p>
</li>
</ul>
<h1 id="linux-io模型">Linux IO模型</h1>
<ul>
<li>1.阻塞IO</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://sparkfengbo.github.io/post-images/1602498833576.jpg" alt="" loading="lazy"></figure>
<p>标红的这部分过程就是阻塞，直到阻塞结束recvfrom才能返回。</p>
<ul>
<li>2.非阻塞IO</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://sparkfengbo.github.io/post-images/1602498843741.jpg" alt="" loading="lazy"></figure>
<p>可以看出recvfrom总是立即返回。</p>
<ul>
<li>3.IO多路复用</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://sparkfengbo.github.io/post-images/1602498851907.jpg" alt="" loading="lazy"></figure>
<ul>
<li>4.信号驱动异步I/O模型</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://sparkfengbo.github.io/post-images/1602498859440.jpg" alt="" loading="lazy"></figure>
<ul>
<li>5.异步IO</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://sparkfengbo.github.io/post-images/1602498867473.jpg" alt="" loading="lazy"></figure>
<ul>
<li>五种模型对比</li>
</ul>
<figure data-type="image" tabindex="6"><img src="https://sparkfengbo.github.io/post-images/1602498874255.jpeg" alt="" loading="lazy"></figure>
<blockquote>
<p>同步非阻塞方式相比同步阻塞方式：<br>
优点是能够在等待任务完成的时间里干其他活了（包括提交其他任务，也就是 “后台” 可以有多个任务在同时执行）。<br>
缺点是任务完成的响应延迟增大了，因为每过一段时间才去轮询一次，而任务可能在两次轮询之间的任意时间完成。<br>
由于同步非阻塞方式需要不断轮询，而 “后台” 可能有多个任务在同时进行，人们就想到了循环查询多个任务的完成状态，只要有任何一个任务完成，就去处理它。这就是所谓的 “I/O 多路复用”。UNIX/Linux 下的 select、poll、epoll 就是干这个的（epoll 比 poll、select 效率高，做的事情是一样的）。Windows 下则有 WaitForMultipleObjects 和 IO Completion Ports API 与之对应（Windows API 的命名简直甩 POSIX API 几条街有木有！）</p>
</blockquote>
<h1 id="io多路复用">IO多路复用</h1>
<blockquote>
<p>目前支持I/O复用的系统调用有<strong>select、pselect、poll、epoll</strong>，下面几小结分别来学习一下select和epoll的使用。<br>
Linux 2.6之前是select、poll，2.6之后是epoll，Windows是IOCP。</p>
</blockquote>
<p>其实，epoll与select原理类似，只不过，epoll作出了一些重大改进，即：</p>
<ul>
<li>1.支持一个进程打开大数目的socket描述符(FD)</li>
</ul>
<blockquote>
<p>select 最不能忍受的是一个进程所打开的FD是有一定限制的，由FD_SETSIZE设置，默认值是2048。对于那些需要支持的上万连接数目的IM服务器来说显然太少了。这时候你一是可以选择修改这个宏然后重新编译内核，不过资料也同时指出这样会带来网络效率的下降，二是可以选择多进程的解决方案(传统的 Apache方案)，不过虽然linux上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。不过 epoll则没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。</p>
</blockquote>
<ul>
<li>2.IO效率不随FD数目增加而线性下降</li>
</ul>
<blockquote>
<p>传统的select/poll另一个致命弱点就是当你拥有一个很大的socket集合，不过由于网络延时，任一时间只有部分的socket是&quot;活跃&quot;的，但是select/poll每次调用都会线性扫描全部的集合，导致效率呈现线性下降。但是epoll不存在这个问题，它只会对&quot;活跃&quot;的socket进行操作---这是因为在内核实现中epoll是根据每个fd上面的callback函数实现的。那么，只有&quot;活跃&quot;的socket才会主动的去调用 callback函数，其他idle状态socket则不会，在这点上，epoll实现了一个&quot;伪&quot;AIO，因为这时候推动力在os内核。在一些 benchmark中，如果所有的socket基本上都是活跃的---比如一个高速LAN环境，epoll并不比select/poll有什么效率，相反，如果过多使用epoll_ctl,效率相比还有稍微的下降。但是一旦使用idle connections模拟WAN环境,epoll的效率就远在select/poll之上了。</p>
</blockquote>
<ul>
<li>3.使用mmap加速内核与用户空间的消息传递。</li>
</ul>
<blockquote>
<p>这点实际上涉及到epoll的具体实现了。无论是select,poll还是epoll都需要内核把FD消息通知给用户空间，如何避免不必要的内存拷贝就很重要，在这点上，epoll是通过内核于用户空间mmap同一块内存实现的。</p>
</blockquote>
<ul>
<li>4.内核微调</li>
</ul>
<blockquote>
<p>这一点其实不算epoll的优点了，而是整个linux平台的优点。也许你可以怀疑linux平台，但是你无法回避linux平台赋予你微调内核的能力。比如，内核TCP/IP协议栈使用内存池管理sk_buff结构，那么可以在运行时期动态调整这个内存pool(skb_head_pool)的大小 --- 通过echo XXXX&gt;/proc/sys/net/core/hot_list_length完成。再比如listen函数的第2个参数(TCP完成3次握手的数据包队列长度)，也可以根据你平台内存大小动态调整。更甚至在一个数据包面数目巨大但同时每个数据包本身大小却很小的特殊系统上尝试最新的NAPI网卡驱动架构。</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linux 硬链接软链接]]></title>
        <id>https://sparkfengbo.github.io/post/czxt-linux-ying-lian-jie-ruan-lian-jie/</id>
        <link href="https://sparkfengbo.github.io/post/czxt-linux-ying-lian-jie-ruan-lian-jie/">
        </link>
        <updated>2020-10-12T10:32:12.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li><a href="https://www.toutiao.com/i6753065098199171587/?tt_from=weixin&amp;utm_campaign=client_share&amp;wxshare_count=1&amp;timestamp=1584238470&amp;app=news_article&amp;utm_source=weixin&amp;utm_medium=toutiao_ios&amp;req_id=2020031510142901001404707528B359F1&amp;group_id=6753065098199171587">面试 | Linux 下软链接和硬链接的区别</a></li>
</ul>
<h1 id="硬链接">硬链接</h1>
<figure data-type="image" tabindex="1"><img src="https://sparkfengbo.github.io/post-images/1602498757010.png" alt="" loading="lazy"></figure>
<p>硬链接，inode节点指向的是同一个文件块<br>
<img src="https://sparkfengbo.github.io/post-images/1602498766032.jpeg" alt="" loading="lazy"></p>
<p>硬链接的作用是允许一个文件拥有多个有效路径名，这样用户就可以建立硬链接到重要文件,以防止“误删”的功能，由于对应该目录的索引节点有一个以上的连接，假设我们删除了原始的 foo.txt 文件：<br>
<img src="https://sparkfengbo.github.io/post-images/1602498773307.jpeg" alt="" loading="lazy"></p>
<p>此时文件的内容依然存在，所以只删除一个连接并不影响索引节点本身和其它的连接，只有当最后一个链接被删除后，文件的数据块及目录的连接才会被释放，也就是说，文件才会被真正删除。</p>
<h1 id="软链接">软链接</h1>
<figure data-type="image" tabindex="2"><img src="https://sparkfengbo.github.io/post-images/1602498779655.jpeg" alt="" loading="lazy"></figure>
<p>软链接又叫符号链接，这个文件包含了另一个文件的路径名，例如在上图中，foo.txt 就是 bar.txt 的软连接，bar.txt 是实际的文件，foo.txt 包含的是对于 bar.txt 的 inode 的记录。</p>
<p>软连接可以是任意文件或目录，可以链接不同文件系统的文件，在对符号文件进行读或写操作的时候，系统会自动把该操作转换为对源文件的操作，但删除链接文件时，系统仅仅删除链接文件，而不删除源文件本身，这一点类似于 Windows 操作系统下的快捷方式。<br>
<img src="https://sparkfengbo.github.io/post-images/1602498790074.jpeg" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linux 信号]]></title>
        <id>https://sparkfengbo.github.io/post/czxt-linux-xin-hao/</id>
        <link href="https://sparkfengbo.github.io/post/czxt-linux-xin-hao/">
        </link>
        <updated>2020-10-12T10:31:50.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li><a href="https://blog.csdn.net/colzer/article/details/8135542">linux基础编程：进程通信之信号</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linux 静态链接和动态链接]]></title>
        <id>https://sparkfengbo.github.io/post/czxt-linux-jing-tai-lian-jie-he-dong-tai-lian-jie/</id>
        <link href="https://sparkfengbo.github.io/post/czxt-linux-jing-tai-lian-jie-he-dong-tai-lian-jie/">
        </link>
        <updated>2020-10-12T10:30:43.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li><a href="https://www.toutiao.com/i6792927363316318723/?group_id=6792927363316318723">C程序的编译过程</a></li>
<li><a href="https://www.toutiao.com/i6793307776551485959/?tt_from=weixin&amp;utm_campaign=client_share&amp;wxshare_count=1&amp;timestamp=1584501857&amp;app=news_article&amp;utm_source=weixin&amp;utm_medium=toutiao_ios&amp;req_id=2020031811241601001404707704B74596&amp;group_id=6793307776551485959">静态链接与动态链接</a></li>
</ul>
<h1 id="c程序编译过程">C程序编译过程</h1>
<figure data-type="image" tabindex="1"><img src="https://sparkfengbo.github.io/post-images/1602498662494.jpeg" alt="" loading="lazy"></figure>
<h2 id="1预处理preprpcessing">1.预处理（Preprpcessing）</h2>
<p><strong>使用预处理器把源文件test.c经过预处理生成test.i文件，预处理用于将所有的#include头文件以及宏定义替换成其真正的内容。</strong></p>
<pre><code>gcc -E test.c -o test.i
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://sparkfengbo.github.io/post-images/1602498670125.png" alt="" loading="lazy"></figure>
<h2 id="2编译compilation">2.编译（Compilation）</h2>
<p><strong>使用编译器将预处理文件test.i编译成汇编文件test.s。</strong></p>
<pre><code>gcc -S test.i -o test.s
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://sparkfengbo.github.io/post-images/1602498677324.png" alt="" loading="lazy"></figure>
<h2 id="3汇编assemble">3.汇编（Assemble）</h2>
<p><strong>使用汇编器将汇编文件test.s转换成目标文件test.o。</strong></p>
<pre><code>gcc -c test.s -o test.o
</code></pre>
<figure data-type="image" tabindex="4"><img src="https://sparkfengbo.github.io/post-images/1602498684007.png" alt="" loading="lazy"></figure>
<h2 id="4链接linking">4.链接（Linking）</h2>
<p><strong>链接过程使用链接器将该目标文件与其他目标文件、库文件、启动文件等链接起来生成可执行文件。</strong></p>
<pre><code>gcc test.o -o test.exe
</code></pre>
<hr>
<h1 id="静态-动态链接">静态、动态链接？</h1>
<ul>
<li>1、什么是静态链接？</li>
</ul>
<p>静态链接是由链接器在链接时将库的内容加入到可执行程序中的做法。链接器是一个独立程序，将一个或多个库或目标文件（先前由编译器或汇编器生成）链接到一块生成可执行程序。这里的库指的是静态链接库，Windows下以.lib为后缀，Linux下以.a为后缀。</p>
<ul>
<li>2、什么是动态链接？</li>
</ul>
<p>动态链接（Dynamic Linking），把链接这个过程推迟到了运行时再进行，在可执行文件装载时或运行时，由操作系统的装载程序加载库。这里的库指的是动态链接库，Windows下以.dll为后缀，Linux下以.so为后缀。值得一提的是，在Windows下的动态链接也可以用到.lib为后缀的文件，但这里的.lib文件叫做导入库，是由.dll文件生成的。</p>
<h2 id="实验">实验</h2>
<p>文件1（main.c）：</p>
<pre><code>#include &quot;test.h&quot;

int main(void)
{
    print_hello();
    system(&quot;pause&quot;);
    return 0;
}
</code></pre>
<p>文件2（test.c）：</p>
<pre><code>#include &quot;test.h&quot;

void print_hello(void)
{
    printf(&quot;hello world\n&quot;);
}
</code></pre>
<p>文件3（test.h）：</p>
<pre><code>#ifndef __TEST_H
#define __TEST_H

#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

void print_hello(void);

#endif
</code></pre>
<h3 id="静态链接实验">静态链接实验</h3>
<ul>
<li>
<p>编译</p>
<pre><code>gcc -c test.c main.c
</code></pre>
<p>多出了test.o和main.o文件</p>
</li>
<li>
<p>静态链接库<br>
接下来使用ar工具把test.o和main.o打包成一个静态库文件lib_test.lib</p>
<pre><code>ar rv lib_test.lib test.o main.o
</code></pre>
</li>
<li>
<p>链接<br>
把这个静态库链接成可执行文件lib_test.exe</p>
<pre><code>gcc lib_test.lib -o lib_test.exe
</code></pre>
</li>
</ul>
<p>可以直接执行lib_test.exe</p>
<h3 id="动态链接实验">动态链接实验</h3>
<ul>
<li>
<p>动态链接库</p>
<pre><code>gcc test.c -shared -o dll_test.dll 
</code></pre>
<pre><code>  多出了动态库文件dll_test.dll
</code></pre>
</li>
<li>
<p>生成可执行文件<br>
用该动态库文件dll_test.dll与main.c一起编译生成可执行文件dll_test.exe</p>
<pre><code>gcc dll_test.dll main.c -o dll_test.exe
</code></pre>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linux 管道]]></title>
        <id>https://sparkfengbo.github.io/post/czxt-linux-guan-dao/</id>
        <link href="https://sparkfengbo.github.io/post/czxt-linux-guan-dao/">
        </link>
        <updated>2020-10-12T10:30:17.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li><a href="https://oilbeater.com/linux/2012/05/29/linux_pipe.html">linux管道机制简介</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[6.安全]]></title>
        <id>https://sparkfengbo.github.io/post/czxt-6an-quan/</id>
        <link href="https://sparkfengbo.github.io/post/czxt-6an-quan/">
        </link>
        <updated>2020-10-12T10:29:46.000Z</updated>
        <content type="html"><![CDATA[<p>这一部分的内容和HTTPS重合</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[5.1.mmap内存映射]]></title>
        <id>https://sparkfengbo.github.io/post/czxt-51mmap-nei-cun-ying-she/</id>
        <link href="https://sparkfengbo.github.io/post/czxt-51mmap-nei-cun-ying-she/">
        </link>
        <updated>2020-10-12T10:28:54.000Z</updated>
    </entry>
    <entry>
        <title type="html"><![CDATA[5.0输入输出和IO(包含零拷贝)]]></title>
        <id>https://sparkfengbo.github.io/post/czxt-50-shu-ru-shu-chu-he-iobao-han-ling-kao-bei/</id>
        <link href="https://sparkfengbo.github.io/post/czxt-50-shu-ru-shu-chu-he-iobao-han-ling-kao-bei/">
        </link>
        <updated>2020-10-12T10:26:00.000Z</updated>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h1 id="1io硬件">1.IO硬件</h1>
<h2 id="分类">分类</h2>
<ul>
<li>块设备 block device
<ul>
<li>块设备把信息存储在固定大小的块中，每个块有自己的地址</li>
<li>CR-ROM、USB</li>
</ul>
</li>
<li>字符设备 character device
<ul>
<li>字符设备以字符为单位发送或接收一个字符流，而不考虑任何块结构。字符设备是不可寻址的，也没有任何寻道操作。</li>
<li>打印机</li>
<li>网络接口</li>
<li>鼠标（用作指点设备）</li>
<li>以及大多数与磁盘不同的设备都可看作是字符设备。</li>
</ul>
</li>
</ul>
<h2 id="设备控制器">设备控制器</h2>
<h2 id="内存映射io">内存映射IO</h2>
<p>优点：</p>
<ul>
<li>对于内存映射I/O，I/O设备驱动程序可以完全用C语言编写。如果不使用内存映射I/O，就要用到某些汇编代码。</li>
<li>不需要特殊的保护机制来阻止用户进程执行I/O操作</li>
<li>可以引用内存的每一条指令也可以引用控制寄存器。</li>
</ul>
<hr>
<h1 id="2io软件">2.IO软件</h1>
<h2 id="程序控制io">程序控制IO</h2>
<p>CPU要不断地查询设备以了解它是否就绪准备接收另一个字符。这一行为经常称为轮询（polling）或忙等待（busy waiting）。</p>
<p>十分简单但是有缺点，即直到全部I/O完成之前要占用CPU的全部时间。</p>
<h2 id="中断驱动io">中断驱动IO</h2>
<figure data-type="image" tabindex="1"><img src="https://sparkfengbo.github.io/post-images/1602498378648.png" alt="" loading="lazy"></figure>
<p>中断驱动I/O的一个明显缺点是中断发生在每个字符上。中断要花费时间，所以这一方法将浪费一定数量的CPU时间。</p>
<h2 id="中断io">中断IO</h2>
<figure data-type="image" tabindex="2"><img src="https://sparkfengbo.github.io/post-images/1602498388360.png" alt="" loading="lazy"></figure>
<ul>
<li>用户进程向 CPU 发起 read 系统调用读取数据，由用户态切换为内核态，然后一直阻塞等待数据的返回。</li>
<li>CPU 在接收到指令以后对磁盘发起 I/O 请求，将磁盘数据先放入磁盘控制器缓冲区。<br>
数据准备完成以后，磁盘向 CPU 发起 I/O 中断。</li>
<li>CPU 收到 I/O 中断以后将磁盘缓冲区中的数据拷贝到内核缓冲区，然后再从内核缓冲区拷贝到用户缓冲区。</li>
<li>用户进程由内核态切换回用户态，解除阻塞状态，然后等待 CPU 的下一个执行时间钟。</li>
</ul>
<p><strong>2次上下文切换、2次拷贝</strong></p>
<h2 id="使用dma的io">使用DMA的IO</h2>
<p>DMA重大的成功是将中断的次数从打印每个字符一次减少到打印每个缓冲区一次。如果有许多字符并且中断十分缓慢，那么采用DMA可能是重要的改进。另一方面，DMA控制器通常比主CPU要慢很多。如果DMA控制器不能以全速驱动设备，或者CPU在等待DMA中断的同时没有其他事情要做，那么采用中断驱动I/O甚至采用程序控制I/O也许更好。</p>
<h2 id="直接存储器存取-dma">直接存储器存取 DMA</h2>
<p>Direct Memory Access，直接内存存取</p>
<p>无论DMA控制器在物理上处于什么地方，它都能够独立于CPU而访问系统总线</p>
<figure data-type="image" tabindex="3"><img src="https://sparkfengbo.github.io/post-images/1602498401543.png" alt="" loading="lazy"></figure>
<p>工作原理：</p>
<ul>
<li>没有DMA</li>
</ul>
<blockquote>
<p>“我们首先看一下没有使用DMA时磁盘如何读。首先，控制器从磁盘驱动器串行地、一位一位地读一个块（一个或多个扇区），直到将整块信息放入控制器的内部缓冲区中。接着，它计算校验和，以保证没有读错误发生。然后控制器产生一个中断。当操作系统开始运行时，它重复地从控制器的缓冲区中一次一个字节或一个字地读取该块的信息，并将其存入内存中。”</p>
</blockquote>
<ul>
<li>有DMA</li>
</ul>
<blockquote>
<p>首先，CPU通过设置DMA控制器的寄存器对它进行编程，所以DMA控制器知道将什么数据传送到什么地方（图5-4中的第1步）。DMA控制器还要向磁盘控制器发出一个命令，通知它从磁盘读数据到其内部的缓冲区中，并且对校验和进行检验。如果磁盘控制器的缓冲区中的数据是有效的，那么DMA就可以开始了。</p>
<p>DMA控制器通过在总线上发出一个读请求到磁盘控制器而发起DMA传送（第2步）。这一读请求看起来与任何其他读请求是一样的，并且磁盘控制器并不知道或者并不关心它是来自CPU还是来自DMA控制器。一般情况下，要写的内存地址在总线的地址线上，所以当磁盘控制器从其内部缓冲区中读取下一个字的时候，它知道将该字写到什么地方。写到内存是另一个标准总线周期（第3步）。当写操作完成时，磁盘控制器在总线上发出一个应答信号到DMA控制器（第4步）。于是，DMA控制器步增要使用的内存地址，并且步减字节计数。如果字节计数仍然大于0，则重复第2步到第4步，直到字节计数到达0。此时，DMA控制器将中断CPU以便让CPU知道传送现在已经完成了。当操作系统开始工作时，用不着将磁盘块复制到内存中，因为它已经在内存中了。</p>
</blockquote>
<blockquote>
<p>普通中断方式是在数据缓冲寄存器满后，发中断请求，CPU进行中断处理<br>
DMA方式则是以数据块为单位传输的,在所要求传送的数据块全部传送结束时要求CPU进行中断处理,大大减少了CPU进行中断处理的次数<br>
总结:DMA方式不需CPU干预传送操作,仅仅是开始和结尾借用CPU一点时间,其余不占用CPU任何资源，中断方式是程序切换,每次操作需要保护和恢复现场</p>
<p>中断控制方式虽然在某种程度上解决了上述问题，但由于中断次数多，因而CPU仍需要花较多的时间处理中断，而且能够并行操作的设备台数也受到中断处理时间的限制，中断次数增多导致数据丢失。</p>
<p>DMA方式和通道方式较好地解决了上述问题。这两种方式采用了外设和内存直接交换数据的方式。只有在一段数据传送结束时，这两种方式才发出中断信号要求CPU做善后处理，从而大大减少了CPU的工作负担。中断控制方式虽然在某种程度上解决了上述问题，但由于中断次数多，因而CPU仍需要花较多的时间处理中断，而且能够并行操作的设备台数也受到中断处理时间的限制，中断次数增多导致数据丢失。DMA方式和通道方式较好地解决了上述问题。这两种方式采用了外设和内存直接交换数据的方式。只有在一段数据传送结束时，这两种方式才发出中断信号要求CPU做善后处理，从而大大减少了CPU的工作负担。</p>
</blockquote>
<p>参考文章</p>
<ul>
<li><a href="https://byte.baike.com/cwiki/DMA&amp;fr=toutiao?isPreloadWebView=1">DMA-字节百科</a></li>
</ul>
<blockquote>
<p>DMA 传输将数据从一个地址空间复制到另外一个地址空间。当CPU 初始化这个传输动作，传输动作本身是由 DMA 控制器来实行和完成。典型的例子就是移动一个外部内存的区块到芯片内部更快的内存区。像是这样的操作并没有让处理器工作拖延，反而可以被重新排程去处理其他的工作。DMA 传输对于高效能 嵌入式系统算法和网络是很重要的。</p>
<p>在实现DMA传输时，是由DMA控制器直接掌管总线，因此，存在着一个总线控制权转移问题。即DMA传输前，CPU要把总线控制权交给DMA控制器，而在结束DMA传输后，DMA控制器应立即把总线控制权再交回给CPU。一个完整的DMA传输过程必须经过DMA请求、DMA响应、DMA传输、DMA结束4个步骤。</p>
<ul>
<li>请求
<ul>
<li>CPU对DMA控制器初始化，并向I/O接口发出操作命令，I/O接口提出DMA请求。</li>
</ul>
</li>
<li>响应
<ul>
<li>DMA控制器对DMA请求判别优先级及屏蔽，向总线裁决逻辑提出总线请求。当CPU执行完当前总线周期即可释放总线控制权。此时，总线裁决逻辑输出总线应答，表示DMA已经响应，通过DMA控制器通知I/O接口开始DMA传输。</li>
</ul>
</li>
<li>传输
<ul>
<li>DMA控制器获得总线控制权后，CPU即刻挂起或只执行内部操作，由DMA控制器输出读写命令，直接控制RAM与I/O接口进行DMA传输。</li>
<li>在DMA控制器的控制下，在存储器和外部设备之间直接进行数据传送，在传送过程中不需要中央处理器的参与。开始时需提供要传送的数据的起始位置和数据长度。</li>
</ul>
</li>
<li>结束
<ul>
<li>当完成规定的成批数据传送后，DMA控制器即释放总线控制权，并向I/O接口发出结束信号。当I/O接口收到结束信号后，一方面停 止I/O设备的工作，另一方面向CPU提出中断请求，使CPU从不介入的状态解脱，并执行一段检查本次DMA传输操作正确性的代码。最后，带着本次操作结果及状态继续执行原来的程序。</li>
</ul>
</li>
</ul>
<p>由此可见，DMA传输方式无需CPU直接控制传输，也没有中断处理方式那样保留现场和恢复现场的过程，通过硬件为RAM与I/O设备开辟一条直接传送数据的通路，使CPU的效率大为提高。</p>
<p>DMA控制器与CPU怎样分时使用内存呢?通常采用以下三种方法：(1)停止CPU访内存；(2)周期挪用；(3)DMA与CPU交替访问内存。</p>
</blockquote>
<figure data-type="image" tabindex="4"><img src="https://sparkfengbo.github.io/post-images/1602498415009.png" alt="" loading="lazy"></figure>
<ul>
<li>用户进程向 CPU 发起 read 系统调用读取数据，由用户态切换为内核态，然后一直阻塞等待数据的返回。</li>
<li>CPU 在接收到指令以后对 DMA 磁盘控制器发起调度指令。</li>
<li>DMA 磁盘控制器对磁盘发起 I/O 请求，将磁盘数据先放入磁盘控制器缓冲区，CPU 全程不参与此过程。</li>
<li>数据读取完成后，DMA 磁盘控制器会接受到磁盘的通知，将数据从磁盘控制器缓冲区拷贝到内核缓冲区。</li>
<li>DMA 磁盘控制器向 CPU 发出数据读完的信号，由 CPU 负责将数据从内核缓冲区拷贝到用户缓冲区。</li>
<li>用户进程由内核态切换回用户态，解除阻塞状态，然后等待 CPU 的下一个执行时间钟。</li>
</ul>
<p><strong>2次上下文切换，2次拷贝</strong></p>
<h2 id="零拷贝">零拷贝</h2>
<ul>
<li><a href="https://www.toutiao.com/i6802161744752935436/?tt_from=weixin&amp;utm_campaign=client_share&amp;wxshare_count=1&amp;timestamp=1583802979&amp;app=news_article&amp;utm_source=weixin&amp;utm_medium=toutiao_android&amp;req_id=202003100916180100260771961F64434A&amp;group_id=6802161744752935436">深入剖析神秘的“零拷贝”「转」</a></li>
<li>强烈推荐这篇文章 <a href="https://juejin.im/post/5d84bd1f6fb9a06b2d780df7">深入剖析Linux IO原理和几种零拷贝机制的实现</a></li>
</ul>
<h3 id="传统io">传统IO</h3>
<h4 id="1传统读写">1.传统读写</h4>
<pre><code>read(file_fd, tmp_buf, len);
write(socket_fd, tmp_buf, len);
</code></pre>
<figure data-type="image" tabindex="5"><img src="https://sparkfengbo.github.io/post-images/1602498425858.png" alt="" loading="lazy"></figure>
<p><strong>整个过程涉及 2 次 CPU 拷贝、2 次 DMA 拷贝总共 4 次拷贝，以及 4 次上下文切换</strong></p>
<ul>
<li>上下文切换：当用户程序向内核发起系统调用时，CPU 将用户进程从用户态切换到内核态；当系统调用返回时，CPU 将用户进程从内核态切换回用户态。</li>
<li>CPU拷贝：由 CPU 直接处理数据的传送，数据拷贝时会一直占用 CPU 的资源。</li>
<li>DMA拷贝：由 CPU 向DMA磁盘控制器下达指令，让 DMA 控制器来处理数据的传送，数据传送完毕再把信息反馈给 CPU，从而减轻了 CPU 资源的占有率。</li>
</ul>
<h4 id="2传统读">2.传统读</h4>
<pre><code>read(file_fd, tmp_buf, len);
</code></pre>
<p><strong>2 次上下文切换，1 次 DMA 拷贝和 1 次 CPU 拷贝</strong></p>
<ul>
<li>用户进程通过 read() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。</li>
<li>CPU利用DMA控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。</li>
<li>CPU将读缓冲区（read buffer）中的数据拷贝到用户空间（user space）的用户缓冲区（user buffer）。</li>
<li>上下文从内核态（kernel space）切换回用户态（user space），read 调用执行返回。</li>
</ul>
<h4 id="3传统写">3.传统写</h4>
<pre><code>write(socket_fd, tmp_buf, len);
</code></pre>
<p><strong>2 次上下文切换，1 次 CPU 拷贝和 1 次 DMA 拷贝</strong></p>
<ul>
<li>户进程通过 write() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。</li>
<li>CPU 将用户缓冲区（user buffer）中的数据拷贝到内核空间（kernel space）的网络缓冲区（socket buffer）。</li>
<li>CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。</li>
<li>上下文从内核态（kernel space）切换回用户态（user space），write 系统调用执行返回。</li>
</ul>
<h3 id="零拷贝-2">零拷贝</h3>
<p>在 Linux 中零拷贝技术主要有 3 个实现思路：用户态直接 I/O、减少数据拷贝次数以及写时复制技术。</p>
<ul>
<li>用户态直接 I/O：应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，硬件上的数据直接拷贝至了用户空间，不经过内核空间。因此，直接 I/O 不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。</li>
<li>减少数据拷贝次数：在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，以及数据在系统内核空间内的CPU拷贝，这也是当前主流零拷贝技术的实现思路。</li>
<li>写时复制技术：写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么将其拷贝到自己的进程地址空间中，如果只是数据读取操作则不需要进行拷贝操作。</li>
</ul>
<h4 id="用户态直接-io">用户态直接 I/O</h4>
<figure data-type="image" tabindex="6"><img src="https://sparkfengbo.github.io/post-images/1602498439992.png" alt="" loading="lazy"></figure>
<blockquote>
<p>用户态直接 I/O 只能适用于不需要内核缓冲区处理的应用程序，这些应用程序通常在进程地址空间有自己的数据缓存机制，称为自缓存应用程序，如数据库管理系统就是一个代表。其次，这种零拷贝机制会直接操作磁盘 I/O，由于 CPU 和磁盘 I/O 之间的执行时间差距，会造成大量资源的浪费，解决方案是配合异步 I/O 使用。</p>
</blockquote>
<h4 id="mmap-write">mmap + write</h4>
<p><strong>减少了 1 次 CPU 拷贝操作</strong></p>
<p><img src="https://sparkfengbo.github.io/post-images/1602498449207.png" alt="" loading="lazy"><br>
<img src="https://sparkfengbo.github.io/post-images/1602498455475.png" alt="" loading="lazy"></p>
<h4 id="sendfile">sendfile</h4>
<p><strong>sendfile 系统调用在 Linux 内核版本 2.1 中被引入，目的是简化通过网络在两个通道之间进行的数据传输过程。sendfile 系统调用的引入，不仅减少了 CPU 拷贝的次数，还减少了上下文切换的次数</strong></p>
<p><img src="https://sparkfengbo.github.io/post-images/1602498461791.png" alt="" loading="lazy"><br>
<img src="https://sparkfengbo.github.io/post-images/1602498466238.png" alt="" loading="lazy"></p>
<h4 id="sendfile-dma-gather-copy">sendfile + DMA gather copy</h4>
<p><strong>Linux 2.4 版本的内核对 sendfile 系统调用进行修改，为  DMA 拷贝引入了 gather 操作。它将内核空间（kernel space）的读缓冲区（read buffer）中对应的数据描述信息（内存地址、地址偏移量）记录到相应的网络缓冲区（ socket  buffer）中，由 DMA 根据内存地址、地址偏移量将数据批量地从读缓冲区（read buffer）拷贝到网卡设备中，这样就省去了内核空间中仅剩的 1 次 CPU 拷贝操作s</strong></p>
<p><img src="https://sparkfengbo.github.io/post-images/1602498477091.png" alt="" loading="lazy"><br>
<img src="https://sparkfengbo.github.io/post-images/1602498482857.png" alt="" loading="lazy"></p>
<h4 id="splice">splice</h4>
<p><strong>sendfile 只适用于将数据从文件拷贝到 socket 套接字上，同时需要硬件的支持，这也限定了它的使用范围。Linux 在 2.6.17 版本引入 splice 系统调用，不仅不需要硬件支持，还实现了两个文件描述符之间的数据零拷贝。</strong></p>
<pre><code>splice(fd_in, off_in, fd_out, off_out, len, flags);
</code></pre>
<p><img src="https://sparkfengbo.github.io/post-images/1602498489058.png" alt="" loading="lazy"><br>
<img src="https://sparkfengbo.github.io/post-images/1602498497440.png" alt="" loading="lazy"></p>
<h4 id="写时复制">写时复制</h4>
<blockquote>
<p>在某些情况下，内核缓冲区可能被多个进程所共享，如果某个进程想要这个共享区进行 write 操作，由于 write 不提供任何的锁操作，那么就会对共享区中的数据造成破坏，写时复制的引入就是 Linux 用来保护数据的。<br>
写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么就需要将其拷贝到自己的进程地址空间中。这样做并不影响其他进程对这块数据的操作，每个进程要修改的时候才会进行拷贝，所以叫写时拷贝。这种方法在某种程度上能够降低系统开销，如果某个进程永远不会对所访问的数据进行更改，那么也就永远不需要拷贝。</p>
</blockquote>
<h4 id="linux零拷贝对比">Linux零拷贝对比</h4>
<figure data-type="image" tabindex="7"><img src="https://sparkfengbo.github.io/post-images/1602498504843.png" alt="" loading="lazy"></figure>
<hr>
<h1 id="3io软件层级">3.IO软件层级</h1>
<h1 id="4盘">4.盘</h1>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[4.存储和文件]]></title>
        <id>https://sparkfengbo.github.io/post/czxt-4cun-chu-he-wen-jian/</id>
        <link href="https://sparkfengbo.github.io/post/czxt-4cun-chu-he-wen-jian/">
        </link>
        <updated>2020-10-12T10:23:50.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1文件">1.文件</h1>
<h2 id="文件命名"><s>文件命名</s></h2>
<h2 id="文件结构"><s>文件结构</s></h2>
<h2 id="文件类型">文件类型</h2>
<ul>
<li>
<p>普通文件</p>
<ul>
<li>普通文件一般分为ASCII文件和二进制文件。ASCII文件由多行正文组成。在某些系统中，每行用回车符结束，其他系统则用换行符结束。有些系统还同时采用回车符和换行符（如MS-DOS）。文件中各行的长度不一定相同。</li>
<li>ASCII文件的最大优势是可以显示和打印，还可以用任何文本编辑器进行编辑。</li>
<li>其他与ASCII文件不同的是二进制文件。打印出来的二进制文件是无法理解的、充满混乱字符的一张表。通常，二进制文件有一定的内部结构，使用该文件的程序才了解这种结构。</li>
</ul>
</li>
<li>
<p>目录</p>
</li>
<li>
<p>字符特殊文件</p>
<ul>
<li>和输入/输出有关，用于串行I/O类设备，如终端、打印机、网络等。</li>
</ul>
</li>
<li>
<p>块特殊文件</p>
<ul>
<li>用于磁盘类设备。</li>
</ul>
</li>
</ul>
<h2 id="文件读取">文件读取</h2>
<ul>
<li><s>顺序存取</s></li>
<li>随机存取</li>
</ul>
<h2 id="文件属性">文件属性</h2>
<blockquote>
<p>文件都有文件名和数据。另外，所有的操作系统还会保存其他与文件相关的信息，如文件创建的日期和时间、文件大小等。这些附加信息称为文件属性（attribute），有些人称之为元数据（metadata）。文件的属性在不同系统中差别很大。一些常用的属性在图4-4中列出，但还存在其他的属性。</p>
</blockquote>
<figure data-type="image" tabindex="1"><img src="https://sparkfengbo.github.io/post-images/1602498249545.png" alt="" loading="lazy"></figure>
<h1 id="2目录">2.目录</h1>
<h2 id="结构">结构</h2>
<ul>
<li>一级目录系统</li>
<li>层次目录系统</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://sparkfengbo.github.io/post-images/1602498258561.png" alt="" loading="lazy"></figure>
<h2 id="路径名">路径名</h2>
<figure data-type="image" tabindex="3"><img src="https://sparkfengbo.github.io/post-images/1602498267327.png" alt="" loading="lazy"></figure>
<h1 id="3文件系统的实现">3.文件系统的实现</h1>
<h2 id="文件系统布局">文件系统布局</h2>
<figure data-type="image" tabindex="4"><img src="https://sparkfengbo.github.io/post-images/1602498275229.png" alt="" loading="lazy"></figure>
<blockquote>
<p>多数磁盘划分为一个或多个分区，每个分区中有一个独立的文件系统。磁盘的0号扇区称为主引导记录（Master Boot Record，MBR），用来引导计算机。在MBR的结尾是分区表。该表给出了每个分区的起始和结束地址。表中的一个分区被标记为活动分区。</p>
</blockquote>
<blockquote>
<p>在计算机被引导时，BIOS读入并执行MBR。MBR做的第一件事是确定活动分区，读入它的第一个块，称为引导块（boot block），并执行之。引导块中的程序将装载该分区中的操作系统。为统一起见，每个分区都从一个启动块开始，即使它不含有一个可启动的操作系统。</p>
</blockquote>
<h2 id="文件的实现">文件的实现</h2>
<ul>
<li>连续分配
<ul>
<li>把每个文件作为一连串连续数据块存储在磁盘上</li>
<li>实现简单，记录第一块的磁盘地址和文件的块数即可</li>
<li>读操作更好</li>
<li>缺点： 磁盘碎片</li>
</ul>
</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://sparkfengbo.github.io/post-images/1602498286294.png" alt="" loading="lazy"></figure>
<ul>
<li>
<p>链表分配</p>
<ul>
<li>没有磁盘碎片</li>
</ul>
<blockquote>
<p>与连续分配方案不同，这一方法可以充分利用每个磁盘块。不会因为磁盘碎片（除了最后一块中的内部碎片）而浪费存储空间。同样，在目录项中，只需要存放第一块的磁盘地址，文件的其他块就可以从这个首块地址查找到。</p>
</blockquote>
<ul>
<li>随机存取缓慢，需要链表查找</li>
<li>指针占去了一些字节<br>
<img src="https://sparkfengbo.github.io/post-images/1602498300227.png" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>在内存中采用表的链表分配</p>
<ul>
<li><strong>如果取出每个磁盘块的指针字，把它放在内存的一个表中，就可以解决上述链表的两个不足。</strong></li>
<li>内存中的这样一个表格称为<strong>文件分配表（File Allocation Table，FAT）</strong>。</li>
<li>保留所有磁盘块的链接表的表大小正比于磁盘自身的大小，磁盘越大，这个表可能很大。</li>
</ul>
</li>
</ul>
<figure data-type="image" tabindex="6"><img src="https://sparkfengbo.github.io/post-images/1602498310160.png" alt="" loading="lazy"></figure>
<ul>
<li>i节点
<ul>
<li>给每个文件赋予一个称为i节点（index-node）的数据结构，其中列出了文件属性和文件块的磁盘地址。</li>
<li><strong>只有在对应文件打开时，其i节点才在内存中</strong></li>
<li>i节点机制需要在内存中有一个数组，其大小正比于可能要同时打开的最大文件个数。它与磁盘无关</li>
<li>相对于在内存中采用表的方式而言，这种机制具有很大的优势，即只有在对应文件打开时，其i节点才在内存中<br>
<img src="https://sparkfengbo.github.io/post-images/1602498320660.png" alt="" loading="lazy"></li>
</ul>
</li>
</ul>
<h2 id="目录的实现">目录的实现</h2>
<p>所有目录项大小一样的实现<br>
<img src="https://sparkfengbo.github.io/post-images/1602498329477.png" alt="" loading="lazy"></p>
<p>非-所有目录项大小一样的实现</p>
<p>“有三个文件，project-budget、personnel和foo”<br>
<img src="https://sparkfengbo.github.io/post-images/1602498337576.png" alt="" loading="lazy"></p>
<h2 id="实现">实现</h2>
<ul>
<li><a href="https://zh.wikipedia.org/wiki/NTFS">NTFS</a></li>
<li>FAT</li>
<li>EXT2
<ul>
<li>https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/59.html</li>
</ul>
</li>
<li>HFS （苹果）</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[3.内存]]></title>
        <id>https://sparkfengbo.github.io/post/czxt-3nei-cun/</id>
        <link href="https://sparkfengbo.github.io/post/czxt-3nei-cun/">
        </link>
        <updated>2020-10-12T10:20:49.000Z</updated>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h1 id="1地址空间">1.地址空间</h1>
<p>如果将物理地址空间暴露给进程会有如下问题：</p>
<ul>
<li>如果用户程序可以寻址内存的每个字节，它们就可以很容易地（故意地或偶然地）破坏操作系统，从而使系统慢慢地停止运行</li>
<li>想要同时（如果只有一个CPU就轮流执行）运行多个程序是很困难的</li>
</ul>
<blockquote>
<ul>
<li>地址空间不隔离</li>
<li>内存使用效率低</li>
<li>程序运行的地址不确定</li>
</ul>
</blockquote>
<p>地址空间是一个进程可用于寻址内存的一套地址集合。每个进程都有一个自己的地址空间，并且这个地址空间独立于其他进程的地址空间（除了在一些特殊情况下进程需要共享它们的地址空间外）。</p>
<p><strong>一种简单的解决方案：</strong></p>
<p>基址寄存器和界限寄存器</p>
<h1 id="2空闲内存的管理方法">2.空闲内存的管理方法</h1>
<h2 id="使用位图进行管理">使用位图进行管理</h2>
<p>使用位图方法时，内存可能被划分成小到几个字或大到几千字节的分配单元。每个分配单元对应于位图中的一位，0表示空闲，1表示占用 。</p>
<p>分配单元的大小是一个重要的设计因素。分配单元越小，位图越大。搜索位图也可能比较耗时。<br>
<img src="https://sparkfengbo.github.io/post-images/1602498066200.png" alt="" loading="lazy"></p>
<h2 id="使用链表进行管理">使用链表进行管理</h2>
<p>维护一个记录已分配内存段和空闲内存段的链表。其中链表中的一个结点或者包含一个进程，或者是两个进程间的一个空的空闲区。</p>
<figure data-type="image" tabindex="1"><img src="https://sparkfengbo.github.io/post-images/1602498066200.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://sparkfengbo.github.io/post-images/1602498087712.png" alt="" loading="lazy"></figure>
<ul>
<li>首次适配算法
<ul>
<li>存储管理器沿着段链表进行搜索，直到找到一个足够大的空闲区，除非空闲区大小和要分配的空间大小正好一样，否则将该空闲区分为两部分，一部分供进程使用，另一部分形成新的空闲区。首次适配算法是一种速度很快的算法，因为它尽可能少地搜索链表结点。</li>
</ul>
</li>
<li>最佳适配算法
<ul>
<li>最佳适配算法搜索整个链表（从开始到结束），找出能够容纳进程的最小的空闲区。</li>
</ul>
</li>
<li>最差适配算法
<ul>
<li>即总是分配最大的可用空闲区，使新的空闲区比较大从而可以继续使用</li>
</ul>
</li>
<li>快速适配算法
<ul>
<li>它为那些常用大小的空闲区维护单独的链表。例如，有一个n项的表，该表的第一项是指向大小为4KB的空闲区链表表头的指针，第二项是指向大小为8KB的空闲区链表表头的指针，第三项是指向大小为12KB的空闲区链表表头的指针，以此类推。像21KB这样的空闲区既可以放在20KB的链表中，也可以放在一个专门存放大小比较特别的空闲区的链表中。</li>
</ul>
</li>
</ul>
<h1 id="3处理内存超载的方法">3.处理内存超载的方法</h1>
<p>有两种处理内存超载的通用方法。最简单的策略是<strong>交换（swapping）技术</strong>，即把一个进程完整调入内存，使该进程运行一段时间，然后把它存回磁盘。空闲进程主要存储在磁盘上，所以当它们不运行时就不会占用内存（尽管它们的一些进程会周期性地被唤醒以完成相关工作，然后就又进入睡眠状态）。另一种策略是<strong>虚拟内存（virtual memory）</strong></p>
<h1 id="4虚拟内存">4.虚拟内存</h1>
<p>虚拟内存的基本思想是：每个程序拥有自己的地址空间，这个空间被分割成多个块，每一块称作一页或页面（page）。每一页有连续的地址范围。这些页被映射到物理内存，但并不是所有的页都必须在内存中才能运行程序。当程序引用到一部分在物理内存中的地址空间时，由硬件立刻执行必要的映射。当程序引用到一部分不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令。</p>
<p>从某个角度来讲，虚拟内存是对基址寄存器和界限寄存器的一种综合。</p>
<h2 id="41-分页">4.1 分页</h2>
<h3 id="mmu-内存管理单元memory-management-unitmmu">MMU - 内存管理单元（Memory Management Unit，MMU）</h3>
<blockquote>
<p>在任何一台计算机上，程序引用了一组内存地址。当程序执行指令</p>
<p>MOV REG,1000</p>
<p>时，它把地址为1000的内存单元的内容复制到REG中（或者相反，这取决于计算机的型号）。地址可以通过索引、基址寄存器、段寄存器或其他方式产生。</p>
<p><strong>由程序产生的这些地址称为虚拟地址（virtual address），它们构成了一个虚拟地址空间（virtual address space）。在没有虚拟内存的计算机上，系统直接将虚拟地址送到内存总线上，读写操作使用具有同样地址的物理内存字；而在使用虚拟内存的情况下，虚拟地址不是被直接送到内存总线上，而是被送到内存管理单元（Memory Management Unit，MMU），MMU把虚拟地址映射为物理内存地址</strong></p>
</blockquote>
<p><strong>缺页中断</strong></p>
<p>MMU注意到该页面没有被映射（在图中用叉号表示），于是使CPU陷入到操作系统，这个陷阱称为缺页中断（page fault）。操作系统找到一个很少使用的页框且把它的内容写入磁盘（如果它不在磁盘上）。随后把需要访问的页面读到刚才回收的页框中，修改映射关系，然后重新启动引起陷阱的指令。</p>
<h3 id="页面和页框">页面和页框</h3>
<p>虚拟地址空间按照固定大小划分成称为页面（page）的若干单元。在物理内存中对应的单元称为页框（page frame）。页面和页框的大小通常是一样的。</p>
<h3 id="工作流程">工作流程</h3>
<p>这个例子中，有一台可以产生16位地址的计算机，地址范围从0到64K，且这些地址是虚拟地址。然而，这台计算机只有32KB的物理内存，因此，虽然可以编写64KB的程序，但它们却不能被完全调入内存运行。在磁盘上必须有一个可以大到64KB的程序核心映像的完整副本，以保证程序片段在需要时能被调入内存。</p>
<figure data-type="image" tabindex="3"><img src="https://sparkfengbo.github.io/post-images/1602498101162.png" alt="" loading="lazy"></figure>
<h2 id="42-页表">4.2 页表</h2>
<blockquote>
<p>虚拟地址被分成虚拟页号（高位部分）和偏移量（低位部分）两部分。</p>
</blockquote>
<p>虚拟地址8196（二进制是0010000000000100）用图3-9所示的MMU映射机制进行映射，输入的16位虚拟地址被分为4位的页号和12位的偏移量。4位的页可以表示16个页面，12位的偏移可以为一页内的全部4096个字节编址。</p>
<p>可用页号作为页表（page table）的索引，以得出对应于该虚拟页面的页框号。如果“在/不在”位是0，则将引起一个操作系统陷阱。如果该位是1，则将在页表中查到的页框号复制到输出寄存器的高3位中，再加上输入虚拟地址中的低12位偏移量。如此就构成了15位的物理地址。输出寄存器的内容随即被作为物理地址送到内存总线。</p>
<figure data-type="image" tabindex="4"><img src="https://sparkfengbo.github.io/post-images/1602498109529.png" alt="" loading="lazy"></figure>
<p><strong>页表项结构</strong></p>
<figure data-type="image" tabindex="5"><img src="https://sparkfengbo.github.io/post-images/1602498116580.png" alt="" loading="lazy"></figure>
<p><strong>大内存分页</strong></p>
<ul>
<li>多级页表
<ul>
<li>引入多级页表的原因是避免把全部页表一直保存在内存中。特别是那些从不需要的页表就不应该保留。</li>
</ul>
</li>
<li>倒排页面
<ul>
<li>在实际内存中每一个页框有一个表项，而不是每一个虚拟页面有一个表项</li>
</ul>
</li>
</ul>
<h2 id="43-加速分页-tlbtranslation-lookaside-buffer">4.3 加速分页 - TLB（translation lookaside buffer）</h2>
<p><strong>问题</strong></p>
<ul>
<li>1)虚拟地址到物理地址的映射必须非常快。</li>
<li>2)如果虚拟地址空间很大，页表也会很大。</li>
</ul>
<blockquote>
<p>最简单的设计（至少从概念上）是使用由一组“快速硬件寄存器”组成的单一页表，每一个表项对应一个虚页，虚页号作为索引，如图3-10所示。当启动一个进程时，操作系统把保存在内存中的进程页表的副本载入到寄存器中。在进程运行过程中，不必再为页表而访问内存。这个方法的优势是简单并且在映射过程中不需要访问内存。而缺点是在页表很大时，代价高昂。而且每一次上下文切换都必须装载整个页表，这样会降低性能。<br>
另一种极端方法是，整个页表都在内存中。那时所需的硬件仅仅是一个指向页表起始位置的寄存器。这样的设计使得在上下文切换时，进行“虚拟地址到物理地址”的映射只需重新装入一个寄存器。当然，这种做法的缺陷是在执行每条指令时，都需要一次或多次内存访问，以完成页表项的读入，速度非常慢。</p>
</blockquote>
<p><strong>转换检测缓冲区</strong></p>
<blockquote>
<p><strong>这种解决方案的建立基于这样一种现象：大多数程序总是对少量的页面进行多次的访问，而不是相反的。因此，只有很少的页表项会被反复读取，而其他的页表项很少被访问。</strong></p>
<p>上面提到的解决方案是为计算机设置一个小型的硬件设备，将虚拟地址直接映射到物理地址，而不必再访问页表。这种设备称为转换检测缓冲区 <strong>（Translation Lookaside Buffer，TLB）</strong>，有时又称为相联存储器（associate memory）<br>
它通常在MMU中，包含少量的表项，在此例中为8个，在实际中很少会超过64个。每个表项记录了一个页面的相关信息，包括虚拟页号、页面的修改位、保护码（读/写/执行权限）和该页所对应的物理页框。除了虚拟页号（不是必须放在页表中的），这些域与页表中的域是一一对应的。另外还有一位用来记录这个表项是否有效（即是否在使用）。</p>
</blockquote>
<figure data-type="image" tabindex="6"><img src="https://sparkfengbo.github.io/post-images/1602498127313.png" alt="" loading="lazy"></figure>
<h1 id="5页面置换算法">5.页面置换算法</h1>
<ul>
<li>
<p>最优页面置换算法</p>
<ul>
<li>无法实现，需要提前知道页面什么时候被访问，将最不可能被访问的页面置换出去</li>
</ul>
</li>
<li>
<p>最近未使用页面置换算法</p>
<ul>
<li>NRU（Not Recently Used）</li>
</ul>
</li>
<li>
<p>先进先出页面置换算法</p>
<ul>
<li>FIFO</li>
<li>简单，FIFO算法可能会把经常使用的页面置换出去”</li>
</ul>
</li>
<li>
<p>第二次机会页面置换算法</p>
<ul>
<li>FIFO的改进算法</li>
<li>检查最老页面的R位。如果R位是0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是1，就将R位清0，并把该页面放到链表的尾端，修改它的装入时间使它就像刚装入的一样，然后继续搜索。</li>
<li>一个比较合理的算法，但它经常要在链表中移动页面，既降低了效率又不是很有必要。</li>
</ul>
</li>
<li>
<p>时钟页面置换算法<br>
<img src="https://sparkfengbo.github.io/post-images/1602498138629.png" alt="" loading="lazy"></p>
</li>
<li>
<p>最近最少使用算法</p>
<ul>
<li>LRU<br>
<img src="https://sparkfengbo.github.io/post-images/1602498149098.png" alt="" loading="lazy"></li>
</ul>
</li>
</ul>
<h1 id="6分段">6.分段</h1>
<p>段表，段基地址，段界限<br>
<img src="https://sparkfengbo.github.io/post-images/1602498156978.png" alt="" loading="lazy"></p>
<p><strong>为什么不用分段？</strong></p>
<p>因为分段粒度太大，以程序的单位，内存不足都是换入换出整个程序，有大量的磁盘访问操作，效率比分页低，实际上分页是从分段发展而来。</p>
]]></content>
    </entry>
</feed>